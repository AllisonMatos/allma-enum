#!/usr/bin/env python3
"""
plugins/urls/main.py - Coleta URLs a partir das URLs vÃ¡lidas do mÃ³dulo domain
e valida novamente com httpx.

SaÃ­das:
  output/<target>/urls/url_completas.txt
  output/<target>/urls/urls_200.txt
"""

from pathlib import Path
import subprocess

from menu import C

from ..output import info, success, warn, error
from .utils import ensure_outdir, require_binary

WANT_STATUS = "200,301,302,307,308"


# ============================================================
# ValidaÃ§Ã£o com httpx
# ============================================================
def httpx_validate(in_file: Path, out_file: Path, want_status: str = WANT_STATUS):
    info(f"{C.BOLD}{C.BLUE}ğŸ” Validando URLs com httpx (mc={want_status})...{C.END}")

    httpx = require_binary("httpx")

    cmd = [
        httpx,
        "-l", str(in_file),
        "-mc", want_status,
        "-retries", "2",
        "-timeout", "15",
        "-random-agent",

        "-follow-redirects",
        "-o", str(out_file),
        "-silent",
    ]

    subprocess.run(cmd)

    if not out_file.exists() or out_file.stat().st_size == 0:
        warn("âš ï¸ Nenhuma URL vÃ¡lida encontrada via httpx.")
        return []

    urls = sorted(
        set(
            l.strip()
            for l in out_file.read_text(errors="ignore").splitlines()
            if l.strip()
        )
    )

    out_file.write_text("\n".join(urls) + "\n")

    success(f"âœ¨ {len(urls)} URLs vÃ¡lidas salvas em: {C.GREEN}{out_file}{C.END}")
    return urls


# ============================================================
# MAIN
# ============================================================
def run(context: dict):
    target = context.get("target")

    if not target:
        raise ValueError("context['target'] Ã© obrigatÃ³rio para o plugin urls")

    # ============================================================
    # ğŸ¯ CABEÃ‡ALHO PREMIUM
    # ============================================================
    info(
        f"\nğŸŸªâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ğŸŸª\n"
        f"   ğŸ”— {C.BOLD}{C.CYAN}INICIANDO MÃ“DULO: URLS{C.END}\n"
        f"   ğŸ¯ Alvo: {C.GREEN}{target}{C.END}\n"
        f"ğŸŸªâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ğŸŸª\n"
    )

    outdir = ensure_outdir(target)
    url_completas = outdir / "url_completas.txt"
    urls_200 = outdir / "urls_200.txt"

    # ============================================================
    # ETAPA 1 â€” Validar arquivo de entrada
    # ============================================================
    domain_200 = Path("output") / target / "domain" / "urls_valid.txt"

    info(f"{C.BOLD}{C.BLUE}ğŸ“„ Verificando arquivo de entrada do mÃ³dulo DOMAIN...{C.END}")

    if not domain_200.exists():
        error(f"âŒ Arquivo de entrada nÃ£o encontrado: {domain_200}")
        return []

    # limpar arquivo anterior
    if url_completas.exists():
        url_completas.unlink()

    # ============================================================
    # ETAPA 1.5 â€” Filtrar URLs estÃ¡ticas (OtimizaÃ§Ã£o)
    # ============================================================
    info(f"{C.BOLD}{C.BLUE}ğŸ§¹ Filtrando arquivos estÃ¡ticos para otimizar urlfinder...{C.END}")
    
    # ExtensÃµes para ignorar no urlfinder (crawling)
    # O usuÃ¡rio pediu especificamente para ignorar JS, mas adicionamos outras estÃ¡ticas
    ignored_exts = {
        ".js", ".css", ".png", ".jpg", ".jpeg", ".gif", ".svg", ".ico", 
        ".woff", ".woff2", ".ttf", ".eot", ".mp4", ".mp3", ".pdf", ".zip", 
        ".rar", ".tar", ".gz", ".7z", ".xml", ".txt", ".json"
    }
    
    urls_to_scan = []
    skipped_count = 0
    
    if domain_200.exists():
        for line in domain_200.read_text(errors="ignore").splitlines():
            line = line.strip()
            if not line: continue
            
            # Verificar extensÃ£o na URL (ignorando query params)
            path = line.split("?")[0].lower()
            if any(path.endswith(ext) for ext in ignored_exts):
                skipped_count += 1
                continue
                
            urls_to_scan.append(line)
            
    urls_filtered_file = outdir / "urls_for_urlfinder.txt"
    urls_filtered_file.write_text("\n".join(urls_to_scan))
    
    info(f"   URLs originais: {len(domain_200.read_text().splitlines())}")
    info(f"   URLs ignoradas: {skipped_count} (arquivos estÃ¡ticos/js)")
    info(f"   URLs para scan: {len(urls_to_scan)}")

    # ============================================================
    # ETAPA 2 â€” Executar urlfinder
    # ============================================================
    info(f"{C.BOLD}{C.BLUE}ğŸŒ Coletando URLs com urlfinder...{C.END}")

    urlfinder = require_binary("urlfinder")
    # Usa o arquivo filtrado em vez do original
    cmd = [urlfinder, "-list", str(urls_filtered_file), "-silent"]

    try:
        with url_completas.open("w", encoding="utf-8", errors="ignore") as fout:
            proc = subprocess.Popen(
                cmd,
                stdout=fout,
                stderr=subprocess.PIPE,
                text=True,
            )
            proc.wait()

    except Exception as e:
        error(f"âŒ Falha ao executar urlfinder: {e}")
        return []

    if not url_completas.exists() or url_completas.stat().st_size == 0:
        warn("âš ï¸ urlfinder nÃ£o retornou nenhuma URL.")
        return []

    # ============================================================
    # ETAPA 3 â€” Deduplicar URLs
    # ============================================================
    info(f"{C.BOLD}{C.BLUE}ğŸ§¹ Deduplicando URLs encontradas...{C.END}")

    lines = [
        l.strip()
        for l in url_completas.read_text(errors="ignore").splitlines()
        if l.strip()
    ]

    unique = sorted(set(lines))
    url_completas.write_text("\n".join(unique) + "\n")

    success(f"ğŸ“ {len(unique)} URLs coletadas em: {C.GREEN}{url_completas}{C.END}")

    # ============================================================
    # ETAPA 4 â€” Validar URLs com httpx
    # ============================================================
    info(f"{C.BOLD}{C.BLUE}ğŸ” Validando URLs com HTTPX...{C.END}")

    valid_urls = httpx_validate(url_completas, urls_200)

    # ============================================================
    # ğŸ‰ FINALIZAÃ‡ÃƒO
    # ============================================================
    success(
        f"\n{C.GREEN}{C.BOLD}âœ” URLS concluÃ­do com sucesso!{C.END}\n"
        f"ğŸ”— URLs vÃ¡lidas: {C.YELLOW}{len(valid_urls)}{C.END}\n"
        f"ğŸ“‚ Arquivo final salvo em:\n"
        f"   {C.CYAN}{urls_200}{C.END}\n"
    )

    return valid_urls
