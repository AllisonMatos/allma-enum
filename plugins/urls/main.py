#!/usr/bin/env python3
import shutil
"""
plugins/urls/main.py - Coleta URLs a partir das URLs v√°lidas do m√≥dulo domain
e valida novamente com httpx.

Sa√≠das:
  output/<target>/urls/url_completas.txt
  output/<target>/urls/urls_200.txt
"""

from pathlib import Path
import subprocess

from menu import C

from ..output import info, success, warn, error
from .utils import ensure_outdir, require_binary

WANT_STATUS = "200,301,302,307,308,401,403,404,405,500"


# ============================================================
# Valida√ß√£o com httpx
# ============================================================
def httpx_validate(in_file: Path, out_file: Path, want_status: str = WANT_STATUS):
    info(f"{C.BOLD}{C.BLUE}üîé Validando URLs com httpx (mc={want_status})...{C.END}")

    httpx = require_binary("httpx")

    cmd = [
        httpx,
        "-l", str(in_file),
        "-mc", want_status,
        "-retries", "2",
        "-timeout", "15",
        "-random-agent",

        "-follow-redirects",
        "-o", str(out_file),
        "-silent",
    ]

    subprocess.run(cmd)

    if not out_file.exists() or out_file.stat().st_size == 0:
        warn("‚ö†Ô∏è Nenhuma URL v√°lida encontrada via httpx.")
        return []

    urls = sorted(
        set(
            l.strip()
            for l in out_file.read_text(errors="ignore").splitlines()
            if l.strip()
        )
    )

    out_file.write_text("\n".join(urls) + "\n")

    success(f"‚ú® {len(urls)} URLs v√°lidas salvas em: {C.GREEN}{out_file}{C.END}")
    return urls


# ============================================================
# Coleta Hist√≥rica (gau / waybackurls)
# ============================================================
def run_historical_discovery(target: str, out_file: Path):
    """
    Executa gau ou waybackurls para encontrar URLs hist√≥ricas.
    """
    info(f"{C.BOLD}{C.BLUE}üï∞Ô∏è Iniciando descoberta de URLs hist√≥ricas...{C.END}")
    
    gau = shutil.which("gau")
    waybackurls = shutil.which("waybackurls")
    tool = gau or waybackurls
    
    if not tool:
        warn("‚ö†Ô∏è Nem 'gau' nem 'waybackurls' encontrados. Pulando hist√≥rico.")
        return []
        
    tool_name = Path(tool).name
    info(f"   üõ†Ô∏è Usando ferramenta: {C.YELLOW}{tool_name}{C.END}")
    
    cmd = [tool, target]
    if "gau" in tool:
        cmd.extend(["--threads", "10"])
        
    try:
        with out_file.open("w", encoding="utf-8", errors="ignore") as fout:
            subprocess.run(cmd, stdout=fout, stderr=subprocess.DEVNULL, timeout=300)
            
        if out_file.exists() and out_file.stat().st_size > 0:
            count = len(out_file.read_text(errors="ignore").splitlines())
            success(f"üìú {count} URLs hist√≥ricas salvas em: {C.GREEN}{out_file.name}{C.END}")
            return [l.strip() for l in out_file.read_text(errors="ignore").splitlines() if l.strip()]
            
    except Exception as e:
        error(f"Erro na coleta hist√≥rica: {e}")
        
    return []


# ============================================================
# MAIN
# ============================================================
def run(context: dict):
    target = context.get("target")

    if not target:
        raise ValueError("context['target'] √© obrigat√≥rio para o plugin urls")

    # ============================================================
    # üéØ CABE√áALHO PREMIUM
    # ============================================================
    info(
        f"\nüü™‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄüü™\n"
        f"   üîó {C.BOLD}{C.CYAN}INICIANDO M√ìDULO: URLS{C.END}\n"
        f"   üéØ Alvo: {C.GREEN}{target}{C.END}\n"
        f"üü™‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄüü™\n"
    )

    outdir = ensure_outdir(target)
    url_completas = outdir / "url_completas.txt"
    urls_200 = outdir / "urls_200.txt"

    # ============================================================
    # ETAPA 1 ‚Äî Coletar URLs de m√∫ltiplas fontes
    # ============================================================
    info(f"{C.BOLD}{C.BLUE}üìÑ Coletando URLs de m√∫ltiplas fontes do pipeline...{C.END}")

    # Fonte 1: URLs validadas do DOMAIN
    domain_200 = Path("output") / target / "domain" / "urls_valid.txt"
    
    # Fonte 2: URLs HTTP descobertas pelo SERVICES (Nmap)
    services_http = Path("output") / target / "services" / "http_urls.txt"
    
    # Fonte 3: URLs descobertas pelo Katana (crawling do DOMAIN)
    katana_valid = Path("output") / target / "domain" / "katana_valid.txt"
    
    # Fonte 4: URLs descobertas inline pelo DOMAIN
    discovered_urls = Path("output") / target / "domain" / "discovered_urls.txt"

    # Coletar todas as seeds
    seed_urls = set()
    sources_found = []
    
    for source_name, source_path in [
        ("domain/urls_valid.txt", domain_200),
        ("services/http_urls.txt", services_http),
        ("domain/katana_valid.txt", katana_valid),
        ("domain/discovered_urls.txt", discovered_urls),
    ]:
        if source_path.exists():
            urls = [l.strip() for l in source_path.read_text(errors="ignore").splitlines() if l.strip()]
            if urls:
                seed_urls.update(urls)
                sources_found.append(f"{source_name} ({len(urls)} URLs)")
                info(f"   ‚úÖ {C.GREEN}{source_name}{C.END}: {len(urls)} URLs")
        else:
            info(f"   ‚ö†Ô∏è {C.YELLOW}{source_name}{C.END}: n√£o encontrado (opcional)")

    if not seed_urls:
        error(f"‚ùå Nenhuma URL seed encontrada de nenhuma fonte!")
        return []
        
    info(f"   üìä {C.CYAN}Total de seeds coletadas: {len(seed_urls)}{C.END}")

    # limpar arquivo anterior
    if url_completas.exists():
        url_completas.unlink()

    # ============================================================
    # ETAPA 1.5 ‚Äî Filtrar URLs est√°ticas (Otimiza√ß√£o)
    # ============================================================
    info(f"{C.BOLD}{C.BLUE}üßπ Filtrando arquivos est√°ticos para otimizar urlfinder...{C.END}")
    
    # Extens√µes para ignorar no urlfinder (crawling)
    # O usu√°rio pediu especificamente para ignorar JS, mas adicionamos outras est√°ticas
    ignored_exts = {
        ".js", ".css", ".png", ".jpg", ".jpeg", ".gif", ".svg", ".ico", 
        ".woff", ".woff2", ".ttf", ".eot", ".mp4", ".mp3", ".pdf", ".zip", 
        ".rar", ".tar", ".gz", ".7z", ".xml", ".txt", ".json"
    }
    
    urls_to_scan = []
    skipped_count = 0
    
    for line in seed_urls:
        # Verificar extens√£o na URL (ignorando query params)
        path = line.split("?")[0].lower()
        if any(path.endswith(ext) for ext in ignored_exts):
            skipped_count += 1
            continue
            
        urls_to_scan.append(line)
            
    urls_filtered_file = outdir / "urls_for_urlfinder.txt"
    urls_filtered_file.write_text("\n".join(urls_to_scan))
    
    info(f"   URLs totais de seeds: {len(seed_urls)}")
    info(f"   URLs ignoradas: {skipped_count} (arquivos est√°ticos/js)")
    info(f"   URLs para scan: {len(urls_to_scan)}")

    # ============================================================
    # ETAPA 2 ‚Äî Executar urlfinder
    # ============================================================
    info(f"{C.BOLD}{C.BLUE}üåê Coletando URLs com urlfinder ({len(urls_to_scan)} seeds)...{C.END}")

    urlfinder = require_binary("urlfinder")
    
    import time as _time
    import tempfile as _tempfile
    
    # Dividir seeds em lotes para mostrar progresso real
    BATCH_SIZE = 100
    total_seeds = len(urls_to_scan)
    url_count = 0
    seeds_done = 0
    start_time = _time.time()
    
    try:
        with url_completas.open("w", encoding="utf-8", errors="ignore") as fout:
            for i in range(0, total_seeds, BATCH_SIZE):
                batch = urls_to_scan[i:i + BATCH_SIZE]
                batch_num = (i // BATCH_SIZE) + 1
                
                # Criar arquivo tempor√°rio para o lote
                tmp = _tempfile.NamedTemporaryFile(mode="w", suffix=".txt", delete=False)
                tmp.write("\n".join(batch) + "\n")
                tmp.close()
                
                cmd = [urlfinder, "-list", tmp.name, "-silent", "-timeout", "10"]
                
                proc = subprocess.Popen(
                    cmd,
                    stdout=subprocess.PIPE,
                    stderr=subprocess.PIPE,
                    text=True,
                )
                
                for line in proc.stdout:
                    fout.write(line)
                    url_count += 1
                
                proc.wait()
                
                # Limpar arquivo tempor√°rio
                try:
                    import os; os.unlink(tmp.name)
                except:
                    pass
                
                seeds_done = min(i + BATCH_SIZE, total_seeds)
                elapsed = _time.time() - start_time
                mins = int(elapsed // 60)
                secs = int(elapsed % 60)
                pct = int(seeds_done / total_seeds * 100)
                
                # Estimar tempo restante
                if seeds_done > 0:
                    eta_secs = (elapsed / seeds_done) * (total_seeds - seeds_done)
                    eta_mins = int(eta_secs // 60)
                    eta_s = int(eta_secs % 60)
                    eta_str = f" | ETA ~{eta_mins}m{eta_s:02d}s"
                else:
                    eta_str = ""
                
                info(f"   ‚è≥ urlfinder: {seeds_done}/{total_seeds} seeds ({pct}%) | {url_count} URLs | {mins}m{secs:02d}s{eta_str}")
        
        elapsed = _time.time() - start_time
        mins = int(elapsed // 60)
        secs = int(elapsed % 60)
        info(f"   ‚úÖ urlfinder conclu√≠do: {url_count} URLs de {total_seeds} seeds em {mins}m{secs:02d}s")

    except Exception as e:
        error(f"‚ùå Falha ao executar urlfinder: {e}")
        # Dont return here, continue to historical
        
    # ============================================================
    # ETAPA 2.5 ‚Äî Coleta Hist√≥rica
    # ============================================================
    historical_file = outdir / "historical_raw.txt"
    run_historical_discovery(target, historical_file)
    
    # Merge files
    all_raw_urls = []
    
    if url_completas.exists():
         all_raw_urls.extend(url_completas.read_text(errors="ignore").splitlines())
         
    if historical_file.exists():
         all_raw_urls.extend(historical_file.read_text(errors="ignore").splitlines())

    if not all_raw_urls:
         warn("‚ö†Ô∏è Nenhuma URL encontrada (urlfinder + hist√≥rico).")
         return []
    
    # Write combined back to url_completas for deduplication
    url_completas.write_text("\n".join(all_raw_urls))

    # ============================================================
    # ETAPA 3 ‚Äî Deduplicar URLs
    # ============================================================
    info(f"{C.BOLD}{C.BLUE}üßπ Deduplicando URLs encontradas...{C.END}")

    lines = [
        l.strip()
        for l in url_completas.read_text(errors="ignore").splitlines()
        if l.strip()
    ]

    unique = sorted(set(lines))
    url_completas.write_text("\n".join(unique) + "\n")

    success(f"üìÅ {len(unique)} URLs coletadas em: {C.GREEN}{url_completas}{C.END}")

    # ============================================================
    # ETAPA 4 ‚Äî Validar URLs com httpx
    # ============================================================
    info(f"{C.BOLD}{C.BLUE}üîç Validando URLs com HTTPX...{C.END}")

    valid_urls = httpx_validate(url_completas, urls_200)

    # ============================================================
    # üéâ FINALIZA√á√ÉO
    # ============================================================
    success(
        f"\n{C.GREEN}{C.BOLD}‚úî URLS conclu√≠do com sucesso!{C.END}\n"
        f"üîó URLs v√°lidas: {C.YELLOW}{len(valid_urls)}{C.END}\n"
        f"üìÇ Arquivo final salvo em:\n"
        f"   {C.CYAN}{urls_200}{C.END}\n"
    )

    return valid_urls
